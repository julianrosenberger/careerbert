{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append('..')  # Add the parent directory to the path\n",
    "from _utils import load_jobs, write_json, load_json, flatten_list\n",
    "from _helpers_parsing import driver_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esco_jobs = load_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eures_overview(esco_jobs, web_driver, output_file, wait_time):\n",
    "    \"\"\"\n",
    "    Scrapes job advertisements from the EURES portal for a given list of ESCO codes.\n",
    "\n",
    "    Parameters:\n",
    "        esco_jobs (list): A list of dictionaries containing ESCO job codes and job titles.\n",
    "        web_driver (webdriver): Selenium WebDriver instance for navigating the EURES portal.\n",
    "        output_file (str): Path to the file where the scraped data will be saved as JSON.\n",
    "        wait_time (int): Maximum wait time (in seconds) for elements to load on the page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing job advertisement details.\n",
    "    \"\"\"\n",
    "    job_data = []\n",
    "\n",
    "    for esco_job in tqdm(esco_jobs):\n",
    "        esco_code = esco_job[\"uri\"].split(\"/\")[-1]\n",
    "        search_url = (\n",
    "            f\"https://europa.eu/eures/portal/jv-se/search?page=1&resultsPerPage=50&orderBy=MOST_RECENT\"\n",
    "            f\"&availableLanguages=de&escoOccupation={esco_code}&lang=de\"\n",
    "        )\n",
    "        web_driver.get(search_url)\n",
    "\n",
    "        # Check if there are no results for the ESCO code\n",
    "        if _check_no_results(web_driver):\n",
    "            continue\n",
    "\n",
    "        # Determine the number of pages to scrape\n",
    "        total_pages = _get_number_of_pages(web_driver, wait_time)\n",
    "        if total_pages is None:\n",
    "            job_data.append({\"searched_esco_job\": esco_job[\"jobtitle\"]})\n",
    "            continue\n",
    "\n",
    "        # Scrape job advertisements from each page\n",
    "        for page_number in range(total_pages):\n",
    "            page_url = (\n",
    "                f\"https://europa.eu/eures/portal/jv-se/search?page={page_number + 1}&resultsPerPage=50\"\n",
    "                f\"&orderBy=MOST_RECENT&availableLanguages=de&escoOccupation={esco_code}&lang=de\"\n",
    "            )\n",
    "            web_driver.get(page_url)\n",
    "            jobs_on_page = _scrape_jobs_from_page(web_driver, wait_time, esco_job[\"jobtitle\"])\n",
    "            job_data.extend(jobs_on_page)\n",
    "\n",
    "        # Save the results to a file\n",
    "        write_json(output_file, job_data)\n",
    "\n",
    "    return job_data\n",
    "\n",
    "\n",
    "def _check_no_results(web_driver):\n",
    "    \"\"\"Check if the search results indicate no jobs found.\"\"\"\n",
    "    try:\n",
    "        WebDriverWait(web_driver, 3).until(EC.presence_of_element_located((By.ID, \"jv-no-result\")))\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _get_number_of_pages(web_driver, wait_time):\n",
    "    \"\"\"Determine the number of pages of job results.\"\"\"\n",
    "    try:\n",
    "        WebDriverWait(web_driver, wait_time).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"ecl-u-type-heading-2\"))\n",
    "        )\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(web_driver.page_source, \"html.parser\")\n",
    "        results_text = soup.find(class_=\"ecl-u-type-heading-2\").text.split(\" \")[2]\n",
    "        total_results = int(\"\".join([char for char in results_text if char.isdigit()]))\n",
    "        total_pages = math.ceil(total_results / 50)\n",
    "        return min(total_pages, 6)  # Limit to the top 300 results (6 pages)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _scrape_jobs_from_page(web_driver, wait_time, searched_job_title):\n",
    "    \"\"\"Scrape job advertisements from the current page.\"\"\"\n",
    "    jobs = []\n",
    "    try:\n",
    "        WebDriverWait(web_driver, wait_time).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"jv-result-job-category\"))\n",
    "        )\n",
    "    except:\n",
    "        return jobs\n",
    "\n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(web_driver.page_source, \"html.parser\")\n",
    "    job_posts = soup.find_all(\"article\")\n",
    "\n",
    "    for job_post in job_posts:\n",
    "        job_url = \"https://europa.eu\" + job_post.find(href=True)[\"href\"]\n",
    "        job_title = job_post.find(href=True).text\n",
    "        publication_date = job_post.find(\"em\").text.replace(\" \", \"\").replace(\"/\", \".\")\n",
    "        esco_categories = [\n",
    "            category.text.strip(\", \")\n",
    "            for category in job_post.find_all(\"span\", {\"class\": \"jv-result-job-category\"})\n",
    "        ]\n",
    "        job_details = {\n",
    "            \"searched_esco_job\": searched_job_title,\n",
    "            \"title\": job_title,\n",
    "            \"url\": job_url,\n",
    "            \"esco_jobs\": esco_categories,\n",
    "            \"publication_date\": publication_date,\n",
    "        }\n",
    "        jobs.append(job_details)\n",
    "\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multithreading the parsing, to speed up the process\n",
    "def multithread_eures_overview(esco_joblist, headless=True, patience=10):\n",
    "    \"\"\"\n",
    "    Multithreaded scraping of job advertisements from the EURES portal.\n",
    "\n",
    "    Parameters:\n",
    "        esco_joblist (list): A list of dictionaries containing ESCO job codes and job titles.\n",
    "        headless (bool): Whether to run the web drivers in headless mode.\n",
    "        patience (int): Maximum wait time (in seconds) for elements to load on the page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing job advertisement details.\n",
    "    \"\"\"\n",
    "    # Set up multiple Selenium WebDriver instances\n",
    "    drivers = [driver_setup(headless) for _ in range(4)]\n",
    "    currently = \"\".join([c for c in str(datetime.now()).split('.')[0] if c.isdigit()])\n",
    "    filenames = [f\"eures_overview/{currently}_eures_overview_{i}.json\" for i in range(1, 5)]\n",
    "    patience_list = [patience for _ in range(4)]\n",
    "\n",
    "    # Split the ESCO job list into chunks for parallel processing\n",
    "    chunks = np.array_split(esco_joblist, 4)\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        bucket = executor.map(get_eures_overview, chunks, drivers, filenames, patience_list)\n",
    "        results = [item for block in bucket for item in block]\n",
    "\n",
    "    # Save the combined results to a single JSON file\n",
    "    write_json(\"eures_overview_total.json\", results)\n",
    "\n",
    "    # Quit all WebDriver instances\n",
    "    [driver.quit() for driver in drivers]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = multithread_eures_overview(esco_jobs, headless=False,patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "parsed_ads = pd.DataFrame(flatten_list([load_json(\"eures_overview/\"+str(x)) for x in os.listdir('eures_overview')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(parsed_ads.drop_duplicates(\"url\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(parsed_ads))\n",
    "parsedads = parsed_ads.drop_duplicates(\"url\")\n",
    "parsedads = parsed_ads.dropna()\n",
    "esco_jobs_parsed = flatten_list([x[\"esco_jobs\"] for x in tqdm(parsed_ads.to_dict(\"records\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(esco_jobs_parsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_ads[\"no_escojobs\"] = parsed_ads[\"esco_jobs\"].apply(len)\n",
    "esco_jobs_parsed = flatten_list([x[\"esco_jobs\"] for x in tqdm(parsed_ads[parsed_ads[\"no_escojobs\"]==1].to_dict(\"records\"))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from _util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(load_json(\"../00_data/EURES/eures_overview_total.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"A total of {len(results)} links to job advertisements were parsed.\")\n",
    "results = results.drop_duplicates([\"title\", \"url\"])\n",
    "print(f\"Without duplicates there are {len(results)} job ad links available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "careerbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
