{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "This section imports all necessary libraries and modules for processing job advertisements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom utilities and helper functions\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from _utils import load_json, write_json\n",
    "from _classification_helpers import setup_classification_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries and third-party modules\n",
    "from nltk import word_tokenize\n",
    "import math\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import util\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to shorten job advertisements by extracting relevant paragraphs\n",
    "def shorten_job_ads(job_ads, classification_pipeline, output_file):\n",
    "    \"\"\"\n",
    "    Shortens job advertisements by processing their text and extracting relevant paragraphs.\n",
    "\n",
    "    Args:\n",
    "        job_ads (list): List of job advertisement texts to be processed.\n",
    "        classification_pipeline (callable): A classification pipeline function to classify text paragraphs.\n",
    "        output_file (str): Path to save the shortened job advertisements as a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing the original job ad and its shortened version.\n",
    "    \"\"\"\n",
    "    shortened_ads = []\n",
    "\n",
    "    for job_ad_text in tqdm(job_ads, desc=\"Processing job ads\"):\n",
    "        annotated_paragraphs = []\n",
    "        # Split the job ad into paragraphs, ignoring empty lines\n",
    "        paragraphs = [line for line in job_ad_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        # If the job ad has no paragraphs, tokenize and split into chunks of 50 tokens\n",
    "        if len(paragraphs) == 1:\n",
    "            tokens = word_tokenize(job_ad_text, language=\"german\")\n",
    "            num_chunks = math.ceil(len(tokens) / 50)\n",
    "            paragraphs = [\" \".join(chunk) for chunk in np.array_split(tokens, num_chunks)]\n",
    "\n",
    "        # Process each paragraph using the classification pipeline\n",
    "        for paragraph in paragraphs:\n",
    "            try:\n",
    "                label = classification_pipeline(paragraph)[0][\"label\"]\n",
    "            except Exception:\n",
    "                # Fallback for long paragraphs\n",
    "                label = classification_pipeline(paragraph[:250])[0][\"label\"]\n",
    "            annotated_paragraphs.append({\"text\": paragraph, \"label\": label})\n",
    "\n",
    "        # Combine paragraphs labeled as \"LABEL_1\" into a shortened version of the job ad\n",
    "        shortened_text = \" \".join(\n",
    "            [annotation[\"text\"] for annotation in annotated_paragraphs if annotation[\"label\"] == \"LABEL_1\"]\n",
    "        )\n",
    "        shortened_ads.append({\"original\": job_ad_text, \"shortened\": shortened_text})\n",
    "\n",
    "        # Save intermediate results to JSON after every 50 job ads\n",
    "        if len(shortened_ads) % 50 == 0:\n",
    "            with open(output_file, 'w', encoding=\"utf-8\") as file:\n",
    "                json.dump(shortened_ads, file, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Save final results to JSON\n",
    "    with open(output_file, 'w', encoding=\"utf-8\") as file:\n",
    "        json.dump(shortened_ads, file, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return shortened_ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to shorten job advertisements using multithreading\n",
    "def multithread_shortening(job_ads):\n",
    "    \"\"\"\n",
    "    Shortens job advertisements using multiple threads for faster processing.\n",
    "\n",
    "    Args:\n",
    "        job_ads (list): List of job advertisement texts to be processed.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing the original job ad and its shortened version.\n",
    "    \"\"\"\n",
    "    classification_pipelines = [setup_classification_pipeline() for _ in range(4)]\n",
    "    job_ad_chunks = np.array_split(job_ads, 4)\n",
    "    timestamp = \"\".join([char for char in str(datetime.now()).split('.')[0] if char.isdigit()])\n",
    "    output_files = [f\"../00_data/EURES/{timestamp}_shortened_ads_{i}.json\" for i in range(1, 5)]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = executor.map(shorten_job_ads, job_ad_chunks, classification_pipelines, output_files)\n",
    "        combined_results = [item for result in results for item in result]\n",
    "\n",
    "    # Save the combined results to a single JSON file\n",
    "    with open(f\"../00_data/EURES/{timestamp}_shortened_ads_total.json\", 'w', encoding=\"utf-8\") as file:\n",
    "        json.dump(combined_results, file, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select random job advertisements for testing\n",
    "def select_random_ads(dataframe, max_per_esco_id):\n",
    "    \"\"\"\n",
    "    Selects random job advertisements for each ESCO ID.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): DataFrame containing job advertisements.\n",
    "        max_per_esco_id (int): Maximum number of job ads to select per ESCO ID.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of randomly selected job advertisements.\n",
    "    \"\"\"\n",
    "    selected_ads = []\n",
    "    unique_esco_ids = dataframe[\"esco_id\"].unique()\n",
    "\n",
    "    for esco_id in tqdm(unique_esco_ids, desc=\"Selecting random ads\"):\n",
    "        filtered_ads = dataframe[dataframe[\"esco_id\"] == esco_id]\n",
    "        if len(filtered_ads) <= max_per_esco_id:\n",
    "            selected_ads += filtered_ads.to_dict(\"records\")\n",
    "        else:\n",
    "            selected_ads += random.sample(filtered_ads.to_dict(\"records\"), max_per_esco_id)\n",
    "\n",
    "    return selected_ads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "This section loads the job advertisements data for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parsed job advertisements from JSON file\n",
    "job_ads = load_json(r\"../00_data/EURES/parsed_ads_final.json\")\n",
    "len(job_ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert job advertisements to a DataFrame and add additional columns\n",
    "job_ads_df = pd.DataFrame(job_ads)\n",
    "job_ads_df.drop([\"count\"], inplace=True, axis=1)\n",
    "job_ads_df[\"num_esco_jobs\"] = job_ads_df[\"esco_jobs\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique job descriptions\n",
    "unique_descriptions = job_ads_df[\"description\"].unique()\n",
    "len(unique_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter unique descriptions\n",
    "unique_descriptions = [desc for desc in tqdm(unique_descriptions) if desc not in unique_descriptions]\n",
    "len(unique_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test job advertisements\n",
    "test_ads = load_json(\"../00_data/EURES/eures_testads_final.json\")\n",
    "\n",
    "# Convert test ads to DataFrame and get unique descriptions\n",
    "test_ads_df = pd.DataFrame(test_ads)\n",
    "unique_descriptions = list(test_ads_df[\"description\"].unique())\n",
    "len(unique_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorten job descriptions using multithreading\n",
    "shortened_descriptions = multithread_shortening(unique_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add shortened descriptions to the DataFrame\n",
    "job_ads_df[\"shortened_texts\"] = shortened_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated job advertisements with shortened descriptions to JSON\n",
    "write_json(\"../00_data/EURES/parsed_shortened_ads_final.json\", job_ads_df.to_dict(\"records\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "careerbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
