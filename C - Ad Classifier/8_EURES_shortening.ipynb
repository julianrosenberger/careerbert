{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _util import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "tqdm.pandas()\n",
    "from sentence_transformers import util\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multithread_shortening(ads):\n",
    "    pipes = [setup_classifier() for _ in range(4)] \n",
    "    chunks = np.array_split(ads, 4)\n",
    "    currently = \"\".join([c for c in str(datetime.now()).split('.')[0] if c.isdigit()])\n",
    "    filenames = [f\"../00_data/EURES/{currently}_eures_ads_shortened{i}.json\" for i in range(1,5)]\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:  \n",
    "        bucket = executor.map(shorten_jobads, chunks,pipes, filenames)\n",
    "        results = [item for block in bucket for item in block]\n",
    "    with open(f\"../00_data/EURES/{currently}_total_ads_shortend.json\", 'w',encoding= \"utf-8\") as fp:\n",
    "        json.dump(results, fp, indent = 2, ensure_ascii=False)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier application\n",
    "def shorten_jobads(ads,pipe,filename):\n",
    "    shortened_texts = []\n",
    "    for ad in tqdm(ads):  \n",
    "        annots_jobad = []\n",
    "        splitted_ad = [x for x in ad.split(\"\\n\") if x != \"\" or x != \" ,\"]\n",
    "        # if no \\n in the ad, tokenize and split at every 50, token\n",
    "        if len(splitted_ad) == 1:\n",
    "            tokenized = word_tokenize(ad, language=\"german\")\n",
    "            no_chunks = math.ceil(len(tokenized)/50)\n",
    "            splitted_ad = np.array_split((tokenized), no_chunks)\n",
    "            splitted_ad = [\" \".join(x) for x in splitted_ad]\n",
    "        for paragraph in splitted_ad:\n",
    "            try:\n",
    "                res = pipe(paragraph)[0][\"label\"]\n",
    "            except:\n",
    "                res = pipe(paragraph[:250])[0][\"label\"]\n",
    "            annots_jobad.append({\"text\":paragraph,\"label\":res})\n",
    "        text_short = \" \".join([x[\"text\"] for x in annots_jobad if x[\"label\"] == \"LABEL_1\"])\n",
    "        shortened_texts.append({ad:text_short})\n",
    "        # safe results to json after every 50th ad\n",
    "        if len(shortened_texts)%50 == 0:\n",
    "            with open(filename, 'w',encoding= \"utf-8\") as fp:\n",
    "                json.dump(shortened_texts, fp, indent = 2, ensure_ascii=False)\n",
    "    # safe final results \n",
    "    with open(filename, 'w',encoding= \"utf-8\") as fp:\n",
    "        json.dump(shortened_texts, fp, indent = 2, ensure_ascii=False)\n",
    "    return shortened_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_random_ads(df, k):\n",
    "    random_choices = []\n",
    "    ids_unique = df[\"esco_id\"].unique()\n",
    "    for id in tqdm(ids_unique):\n",
    "        filtered_df = df[df[\"esco_id\"] == id]\n",
    "        if len(filtered_df) <= k:\n",
    "            random_choices += filtered_df.to_dict(\"records\")\n",
    "        else:\n",
    "            random_choices += random.sample(filtered_df.to_dict(\"records\"), k)\n",
    "    return random_choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads = load_json(r\"../00_data/EURES/parsed_ads_final.json\")\n",
    "len(ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_df = pd.DataFrame(ads)\n",
    "ads_df.drop([\"count\"], inplace=True, axis=1)\n",
    "ads_df[\"count_esco_jobs\"] = ads_df[\"esco_jobs\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_descriptions = ads_df[\"description\"].unique()\n",
    "len(unique_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_descriptions = [x for x in tqdm(unique_descriptions) if x not in processed_descriptions]\n",
    "len(unique_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ads = load_json(\"../00_data/EURES/eures_testads_final.json\")\n",
    "selected_ads = pd.DataFrame(test_ads)\n",
    "unique_descriptions = list(selected_ads[\"description\"].unique())\n",
    "len(unique_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_desc = multithread_shortening(unique_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_ads)\n",
    "len(ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_df = pd.DataFrame(ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {}\n",
    "for item in processed_ads:\n",
    "    replace_dict.update(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_df[\"short_texts\"] = ads_df[\"description\"].map(replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(\"../00_data/EURES/0_pars_short_ads_final.json\", ads_df.to_dict(\"records\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb0bef7a9eff8501420d8e3c6496920c898020e76359dc7d7e37d89f971fe60a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
